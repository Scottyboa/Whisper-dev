<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title id="page-title-transcribe">Transcription Tool with Ads and Guide Overlay</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500&family=Merriweather:wght@300;400&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background-color: #f8f8f8;
      color: #333;
      margin: 0;
      padding: 0;
    }
    /* Style for the OpenAI usage hyperlink */
    #openaiUsageLink {
      position: fixed;
      top: 30px;       
      right: 400px;    
      font-size: 18px; 
      text-decoration: underline;
      color: #0077cc;
      z-index: 1000;
    }
    /* Grid Layout */
    .grid-container {
      display: grid;
      grid-template-columns: 250px 1fr 250px;
      height: 100vh;
    }
    /* Left Sidebar */
    .sidebar {
      background-color: #e0e0e0;
      padding: 20px;
      display: flex;
      flex-direction: column;
      gap: 20px;
      position: relative;
    }
    .sidebar button {
      font-size: 18px;
      padding: 15px;
    }
    /* Main Content Area */
    .main-content {
      padding: 20px;
      display: flex;
      flex-direction: column;
      gap: 20px;
      overflow-y: auto;
    }
    /* Recording Area (Top Half) */
    .recording-area {
      border-bottom: 1px solid #ddd;
      padding-bottom: 20px;
    }
    .timer {
      font-size: 18px;
      margin-bottom: 10px;
      color: #333;
    }
    /* Auto-resizing Textareas */
    #transcription, #generatedNote, #customPrompt {
      width: 100%;
      padding: 12px;
      border: 1px solid #ddd;
      border-radius: 10px;
      font-size: 16px;
      box-sizing: border-box;
      margin-bottom: 20px;
      font-family: 'Roboto', sans-serif;
      resize: none;
    }
    /* Allow vertical resize for the transcription output field */
    #transcription {
      resize: vertical;
    }
    #transcription, #generatedNote {
      min-height: 150px;
    }
    #customPrompt {
      min-height: 200px;
    }
    button {
      background-color: #5a9;
      color: #fff;
      border: none;
      border-radius: 10px;
      padding: 12px 20px;
      font-size: 16px;
      cursor: pointer;
      transition: background-color 0.3s, transform 0.1s;
      margin: 10px;
    }
    button:hover {
      background-color: #489;
    }
    button:active {
      transform: scale(0.98);
    }
    /* Bottom Half: Two Columns */
    .bottom-half {
      display: flex;
      gap: 20px;
      flex: 1;
    }
    .column {
      flex: 1;
      display: flex;
      flex-direction: column;
      gap: 10px;
    }
    /* Right Sidebar (Ad Area) */
    .ad-sidebar {
      background-color: #e0e0e0;
      padding: 20px;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    #adUnit {
      width: 100%;
      height: 200px;
      background-color: #ddd;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 18px;
      margin-bottom: 10px;
    }
    /* Guide Overlay */
    #guideView {
      display: none;
      position: fixed;
      top: 10%;
      left: 10%;
      width: 80%;
      height: 80%;
      background: white;
      border: 2px solid #ccc;
      padding: 20px;
      overflow-y: auto;
      z-index: 2000;
    }
    #guideView.active {
      display: block;
    }
    /* Language Dropdown in Sidebar */
    #lang-container-transcribe {
      position: absolute;
      bottom: 20px;
      left: 20px;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    #lang-icon-transcribe {
      width: 28px;
      height: 28px;
    }
    /* Consent Banner */
    #cmp-banner {
      position: fixed;
      bottom: 0;
      left: 0;
      right: 0;
      background: #f8f8f8;
      border-top: 1px solid #ccc;
      padding: 20px;
      text-align: center;
      font-size: 16px;
      z-index: 1000;
      box-shadow: 0 -2px 5px rgba(0,0,0,0.1);
    }
    #cmp-banner button {
      margin-left: 10px;
      padding: 8px 16px;
    }
  </style>
</head>
<body>
  <!-- OpenAI usage hyperlink -->
  <a id="openaiUsageLink" href="https://platform.openai.com/usage" target="_blank">Cost usage overview</a>
  
  <div class="grid-container">
    <!-- Left Sidebar -->
    <aside class="sidebar">
      <button id="btnFunctions">Functions</button>
      <button id="btnGuide">Guide</button>
      <!-- Language dropdown at bottom of sidebar -->
      <div id="lang-container-transcribe">
        <img src="language-icon.png" alt="Language Icon" id="lang-icon-transcribe">
        <select id="lang-select-transcribe">
          <option value="en">English</option>
          <option value="no">Norsk</option>
          <option value="sv">Svenska</option>
          <option value="zh">中文</option>
          <option value="de">Deutsch</option>
          <option value="fr">Français</option>
        </select>
      </div>
    </aside>
    <!-- Main Content -->
    <main class="main-content">
      <!-- Top Half: Recording Area -->
      <div class="recording-area">
        <h3 id="recordingAreaTitle">Recording Area</h3>
        <div id="recordIndicator" style="width:20px; height:20px; border-radius:50%; background-color:grey; margin:0 auto 10px;"></div>
        <div id="recordTimer" class="timer">Recording Timer: 0 sec</div>
        <div id="uploadTimer" class="timer">Upload Timer: 0 sec</div>
        <div id="transcribeTimer" class="timer">Transcription Timer: 0 sec</div>
        <textarea id="transcription" placeholder="Transcription result will appear here..."></textarea>
        <div>
          <button id="startButton">Start Recording</button>
          <button id="stopButton" disabled>Stop Recording</button>
          <button id="pauseResumeButton" disabled>Pause Recording</button>
          <button id="transcribeButton" disabled>Transcribe</button>
        </div>
        <div id="statusMessage">Welcome! Click "Start Recording" to begin.</div>
      </div>
      <!-- Bottom Half: Note Generation and Custom Prompt -->
      <div class="bottom-half">
        <div class="column">
          <h3 id="noteGenerationTitle">Note Generation</h3>
          <button id="generateNoteButton">Generate Note</button>
          <div id="noteTimer" class="timer">Note Generation Timer: 0 sec</div>
          <textarea id="generatedNote" readonly placeholder="Generated note will appear here..."></textarea>
        </div>
        <div class="column">
          <h3 id="customPromptTitle">Custom Prompt</h3>
          <label for="promptSlot" id="promptSlotLabel">Prompt Slot:</label>
          <select id="promptSlot">
            <option value="1">1</option>
            <option value="2">2</option>
            <option value="3">3</option>
            <option value="4">4</option>
            <option value="5">5</option>
            <option value="6">6</option>
            <option value="7">7</option>
            <option value="8">8</option>
            <option value="9">9</option>
            <option value="10">10</option>
          </select>
          <textarea id="customPrompt" placeholder="Enter custom prompt here" rows="1"></textarea>
        </div>
      </div>
    </main>
    <!-- Right Sidebar (Ad Area) -->
    <aside class="ad-sidebar">
      <div id="adArea">
        <div id="adUnit">Your Ad Here</div>
      </div>
    </aside>
  </div>
  <!-- Guide Overlay -->
  <div id="guideView">
    <h3 id="guideHeading">Guide & Instructions</h3>
    <p id="guideText">
      Welcome to the Whisper Transcription tool. This application allows medical professionals, therapists, and other practitioners to record and transcribe consultations, as well as generate professional notes using an AI-powered note generator.
      <br><br>
      <strong>How to Use the Functions:</strong>
      <ul>
        <li><strong>Recording:</strong> Click "Start Recording" to begin capturing audio. The recording is now continuous using a 2‑minute timeslice, so chunks are automatically generated and sent for transcription without any gap.</li>
        <li><strong>Transcription:</strong> As soon as each chunk is finished, it is uploaded and transcribed in the background. Once you stop recording, the status will update to “Ready to transcribe!” and you can then click "Transcribe" to fetch the compiled transcription.</li>
        <li><strong>Note Generation:</strong> After transcription, click "Generate Note" to produce a note based on your transcription and custom prompt.</li>
        <li><strong>Custom Prompt:</strong> On the right, select a prompt slot (1–10) and enter your custom prompt. Your prompt is saved automatically and linked to your API key.</li>
        <li><strong>Guide Toggle:</strong> Use the "Functions" and "Guide" buttons to switch between the functional view and this guide.</li>
      </ul>
      Please click "Functions" to return to the main interface.
    </p>
  </div>
  <!-- Consent Banner -->
  <div id="cmp-banner">
    <span id="consent-text">This website is free to use because we rely solely on ad revenue. We use cookies to personalize ads and improve your experience. By clicking "Accept", you consent to the use of cookies.</span>
    <button id="cmp-accept">Accept</button>
    <button id="cmp-manage">Manage</button>
  </div>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      /* --------------------------
         Multi-language & Consent Banner Logic
         -------------------------- */
      const transcribeTranslations = {
        en: {
          pageTitle: "Transcription Tool with Ads and Guide Overlay",
          openaiUsageLinkText: "Cost usage overview",
          btnFunctions: "Functions",
          btnGuide: "Guide",
          recordingAreaTitle: "Recording Area",
          recordTimer: "Recording Timer: 0 sec",
          uploadTimer: "Upload Timer: 0 sec",
          transcribeTimer: "Transcription Timer: 0 sec",
          transcriptionPlaceholder: "Transcription result will appear here...",
          startButton: "Start Recording",
          stopButton: "Stop Recording",
          pauseButton: "Pause Recording",
          transcribeButton: "Transcribe",
          statusMessage: "Welcome! Click \"Start Recording\" to begin.",
          noteGenerationTitle: "Note Generation",
          generateNoteButton: "Generate Note",
          noteTimer: "Note Generation Timer: 0 sec",
          generatedNotePlaceholder: "Generated note will appear here...",
          customPromptTitle: "Custom Prompt",
          promptSlotLabel: "Prompt Slot:",
          customPromptPlaceholder: "Enter custom prompt here",
          adUnitText: "Your Ad Here",
          guideHeading: "Guide & Instructions",
          guideText: `Welcome to the Whisper Transcription tool. This application allows medical professionals, therapists, and other practitioners to record and transcribe consultations, as well as generate professional notes using an AI-powered note generator.
<br><br>
<strong>How to Use the Functions:</strong>
<ul>
  <li><strong>Recording:</strong> Click "Start Recording" to begin capturing audio. The recording is continuous using a 2-minute timeslice so that chunks are generated without pause and sent for transcription immediately.</li>
  <li><strong>Transcription:</strong> As each 2-minute chunk (or final shorter chunk) is finished, it is uploaded and transcribed in the background. Once you stop recording, the status will update to “Ready to transcribe!” and you can then click "Transcribe" to fetch the compiled transcription.</li>
  <li><strong>Note Generation:</strong> After transcription, click "Generate Note" to produce a note based on the transcription and your custom prompt.</li>
  <li><strong>Custom Prompt:</strong> On the right, select a prompt slot (1–10) and enter your custom prompt. It is saved automatically and linked to your API key.</li>
  <li><strong>Guide Toggle:</strong> Use "Functions" and "Guide" to switch views.</li>
</ul>
Please click "Functions" to return to the main interface.`
        },
        no: {
          pageTitle: "Transkripsjonsverktøy med annonser og veiledningsoverlegg",
          openaiUsageLinkText: "Vis OpenAI bruk",
          btnFunctions: "Funksjoner",
          btnGuide: "Veiledning",
          recordingAreaTitle: "Opptaksområde",
          recordTimer: "Opptakstimer: 0 sek",
          uploadTimer: "Opplastningstimer: 0 sek",
          transcribeTimer: "Transkripsjonstimer: 0 sek",
          transcriptionPlaceholder: "Transkripsjonsresultatet vises her...",
          startButton: "Start opptak",
          stopButton: "Stopp opptak",
          pauseButton: "Pause opptak",
          transcribeButton: "Transkriber",
          statusMessage: "Velkommen! Klikk 'Start opptak' for å begynne.",
          noteGenerationTitle: "Notatgenerering",
          generateNoteButton: "Generer notat",
          noteTimer: "Notatgenereringstimer: 0 sek",
          generatedNotePlaceholder: "Generert notat vises her...",
          customPromptTitle: "Tilpasset melding",
          promptSlotLabel: "Meldingsplass:",
          customPromptPlaceholder: "Skriv inn tilpasset melding her",
          adUnitText: "Din annonse her",
          guideHeading: "Veiledning og Instruksjoner",
          guideText: `Velkommen til Whisper Transkripsjonsverktøy. Denne applikasjonen lar medisinske fagpersoner, terapeuter og andre utøvere ta opp og transkribere konsultasjoner, samt generere profesjonelle notater ved hjelp av en AI-drevet notatgenerator.
<br><br>
<strong>Slik bruker du verktøyet:</strong>
<ul>
  <li><strong>Opptak:</strong> Klikk "Start opptak" for å starte opptaket. Opptaket kjøres kontinuerlig med 2-minutters segmenter, og hver del sendes automatisk til transkribering.</li>
  <li><strong>Transkripsjon:</strong> Når hvert 2-minutters segment (eller den siste kortere delen) er ferdig, lastes det opp og transkriberes i bakgrunnen. Etter at du stopper opptaket, vil statusen oppdateres til "Klar for transkribering!" og du klikker "Transkriber" for å hente den sammensatte transkripsjonen.</li>
  <li><strong>Notatgenerering:</strong> Etter transkripsjonen klikker du "Generer notat" for å lage et notat basert på transkripsjonen og din tilpassede melding.</li>
  <li><strong>Tilpasset melding:</strong> Velg et meldingsfelt (1–10) og skriv inn din melding. Den lagres automatisk og knyttes til din API-nøkkel.</li>
  <li><strong>Veiledning:</strong> Bruk "Funksjoner" og "Veiledning" for å bytte mellom grensesnitt og veiledning.</li>
</ul>
Klikk "Funksjoner" for å gå tilbake til hovedskjermen.`
        },
        sv: {
          pageTitle: "Transkriptionsverktyg med annonser och guideöverlägg",
          openaiUsageLinkText: "Visa OpenAI användning",
          btnFunctions: "Funktioner",
          btnGuide: "Guide",
          recordingAreaTitle: "Inspelningsområde",
          recordTimer: "Inspelningstimer: 0 sek",
          uploadTimer: "Uppladdningstimer: 0 sek",
          transcribeTimer: "Transkriptionstimer: 0 sek",
          transcriptionPlaceholder: "Transkriptet visas här...",
          startButton: "Starta inspelning",
          stopButton: "Stoppa inspelning",
          pauseButton: "Pausa inspelning",
          transcribeButton: "Transkribera",
          statusMessage: "Välkommen! Klicka på 'Starta inspelning' för att börja.",
          noteGenerationTitle: "Notatgenerering",
          generateNoteButton: "Generera notat",
          noteTimer: "Notatgenereringstimer: 0 sek",
          generatedNotePlaceholder: "Genererat notat visas här...",
          customPromptTitle: "Anpassat meddelande",
          promptSlotLabel: "Meddelandefält:",
          customPromptPlaceholder: "Ange ditt anpassade meddelande här",
          adUnitText: "Din annons här",
          guideHeading: "Guide & Instruktioner",
          guideText: `Välkommen till Whisper Transkriptionsverktyget. Detta verktyg låter medicinska experter, terapeuter och andra utövare spela in och transkribera konsultationer samt generera professionella noteringar med hjälp av en AI-driven notatgenerator.
<br><br>
<strong>Så här använder du verktyget:</strong>
<ul>
  <li><strong>Inspelning:</strong> Klicka på "Starta inspelning" för att börja. Inspelningen sker kontinuerligt med 2-minuters segment som automatiskt skickas för transkribering.</li>
  <li><strong>Transkription:</strong> När varje segment (eller sista korta segmentet) är klart, laddas det upp och transkriberas i bakgrunden. Efter stopp kommer statusen att uppdateras till "Klar för transkribering!" och du kan klicka "Transkribera" för att hämta det sammansatta transkriptet.</li>
  <li><strong>Notatgenerering:</strong> Klicka på "Generera notat" för att skapa ett notat baserat på inspelningen och ditt anpassade meddelande.</li>
  <li><strong>Anpassat meddelande:</strong> Välj ett fält (1–10) och skriv in ditt meddelande. Detta sparas automatiskt och kopplas till din API-nyckel.</li>
  <li><strong>Guide:</strong> Klicka på "Funktioner" för att återgå till huvudsidan.</li>
</ul>
Klicka på "Funktioner" för att återgå till huvudsidan.`
        },
        zh: {
          pageTitle: "转录工具与广告及指南覆盖",
          openaiUsageLinkText: "查看 OpenAI 使用情况",
          btnFunctions: "功能",
          btnGuide: "指南",
          recordingAreaTitle: "录音区域",
          recordTimer: "录音计时器: 0 秒",
          uploadTimer: "上传计时器: 0 秒",
          transcribeTimer: "转录计时器: 0 秒",
          transcriptionPlaceholder: "转录结果将在此显示……",
          startButton: "开始录音",
          stopButton: "停止录音",
          pauseButton: "暂停录音",
          transcribeButton: "转录",
          statusMessage: "欢迎！点击“开始录音”开始录音。",
          noteGenerationTitle: "笔记生成",
          generateNoteButton: "生成笔记",
          noteTimer: "笔记生成计时器: 0 秒",
          generatedNotePlaceholder: "生成的笔记将在此显示……",
          customPromptTitle: "自定义提示",
          promptSlotLabel: "提示槽：",
          customPromptPlaceholder: "在此输入自定义提示",
          adUnitText: "您的广告在此",
          guideHeading: "指南与说明",
          guideText: `欢迎使用 Whisper 转录工具。本工具允许医疗专业人员、治疗师及其他从业者录制并转录咨询内容，同时利用 AI 驱动的笔记生成器生成专业笔记。
<br><br>
<strong>如何使用此工具：</strong>
<ul>
  <li><strong>录音：</strong>点击“开始录音”，录音将以2分钟为片段连续进行，片段结束后自动上传并开始转录。</li>
  <li><strong>转录：</strong>每个2分钟片段（或最后较短的片段）上传后即转录。录音结束后，状态会更新为“转录就绪！”，然后点击“转录”获取全部合成转录文本。</li>
  <li><strong>笔记生成：</strong>录音转录完成后，点击“生成笔记”根据转录文本和自定义提示生成笔记。</li>
  <li><strong>自定义提示：</strong>在右侧选择提示槽（1–10）并输入提示，系统会自动保存并关联您的 API 密钥。</li>
  <li><strong>指南：</strong>使用“功能”和“指南”按钮在功能视图和本指南之间切换。</li>
</ul>
点击“功能”返回主界面。`
        },
        de: {
          pageTitle: "Whisper Transkription",
          openaiUsageLinkText: "OpenAI-Nutzung anzeigen",
          btnFunctions: "Funktionen",
          btnGuide: "Guide",
          recordingAreaTitle: "Aufnahmebereich",
          recordTimer: "Aufnahmetimer: 0 Sek",
          uploadTimer: "Upload-Timer: 0 Sek",
          transcribeTimer: "Transkriptionstimer: 0 Sek",
          transcriptionPlaceholder: "Das Transkript erscheint hier...",
          startButton: "Aufnahme starten",
          stopButton: "Aufnahme stoppen",
          pauseButton: "Aufnahme pausieren",
          transcribeButton: "Transkribieren",
          statusMessage: "Willkommen! Klicken Sie auf 'Aufnahme starten', um zu beginnen.",
          noteGenerationTitle: "Notizgenerierung",
          generateNoteButton: "Notiz generieren",
          noteTimer: "Notizgenerierungstimer: 0 Sek",
          generatedNotePlaceholder: "Die generierte Notiz erscheint hier...",
          customPromptTitle: "Benutzerdefinierte Eingabe",
          promptSlotLabel: "Eingabefeld:",
          customPromptPlaceholder: "Geben Sie hier Ihre benutzerdefinierte Eingabe ein",
          adUnitText: "Ihre Anzeige hier",
          guideHeading: "Guide & Anleitungen",
          guideText: `Willkommen beim Whisper Transkriptionswerkzeug. Diese Anwendung ermöglicht es medizinischen Fachkräften, Therapeuten und anderen Anwendern, Beratungsgespräche aufzunehmen und zu transkribieren sowie professionelle Notizen mit Hilfe eines KI-gestützten Notizgenerators zu erstellen.
<br><br>
<strong>So verwenden Sie das Tool:</strong>
<ul>
  <li><strong>Aufnahme:</strong> Klicken Sie auf "Aufnahme starten", um die Aufnahme zu beginnen. Die Aufnahme erfolgt kontinuierlich in 2-Minuten-Segmenten, die automatisch hochgeladen und transkribiert werden.</li>
  <li><strong>Transkription:</strong> Sobald jedes 2-Minuten-Segment (oder das letzte kürzere Segment) fertig ist, wird es hochgeladen und im Hintergrund transkribiert. Nach dem Stopp wird der Status auf "Bereit zur Transkription!" aktualisiert und Sie können auf "Transkribieren" klicken, um das zusammengesetzte Transkript abzurufen.</li>
  <li><strong>Notizgenerierung:</strong> Nach der Transkription klicken Sie auf "Notiz generieren", um basierend auf dem Transkript und Ihrer benutzerdefinierten Eingabe eine Notiz zu erstellen.</li>
  <li><strong>Benutzerdefinierte Eingabe:</strong> Wählen Sie rechts ein Eingabefeld (1–10) aus und geben Sie Ihre benutzerdefinierte Eingabe ein. Diese wird automatisch gespeichert und mit Ihrem API-Schlüssel verknüpft.</li>
  <li><strong>Guide:</strong> Verwenden Sie die Schaltflächen "Funktionen" und "Guide", um zwischen der funktionsreichen Ansicht und dieser Anleitung zu wechseln.</li>
</ul>
Klicken Sie auf "Funktionen", um zur Hauptansicht zurückzukehren.`
        },
        fr: {
          pageTitle: "Transcription Whisper",
          openaiUsageLinkText: "Voir l'utilisation OpenAI",
          btnFunctions: "Fonctions",
          btnGuide: "Guide",
          recordingAreaTitle: "Zone d'enregistrement",
          recordTimer: "Minuterie d'enregistrement : 0 sec",
          uploadTimer: "Minuterie de chargement : 0 sec",
          transcribeTimer: "Minuterie de transcription : 0 sec",
          transcriptionPlaceholder: "Le résultat de la transcription s'affichera ici…",
          startButton: "Démarrer l'enregistrement",
          stopButton: "Arrêter l'enregistrement",
          pauseButton: "Mettre en pause",
          transcribeButton: "Transcrire",
          statusMessage: "Bienvenue ! Cliquez sur \"Démarrer l'enregistrement\" pour commencer.",
          noteGenerationTitle: "Génération de notes",
          generateNoteButton: "Générer une note",
          noteTimer: "Minuterie de génération de notes : 0 sec",
          generatedNotePlaceholder: "La note générée s'affichera ici…",
          customPromptTitle: "Message personnalisé",
          promptSlotLabel: "Emplacement du message :",
          customPromptPlaceholder: "Entrez votre message personnalisé ici",
          adUnitText: "Votre annonce ici",
          guideHeading: "Guide & Instructions",
          guideText: `Bienvenue sur l'outil de transcription Whisper. Cette application permet aux professionnels de la santé, aux thérapeutes et à d'autres praticiens d'enregistrer et de transcrire des consultations, ainsi que de générer des notes professionnelles à l'aide d'un générateur de notes propulsé par l'IA.
<br><br>
<strong>Comment utiliser l'outil :</strong>
<ul>
  <li><strong>Enregistrement :</strong> Cliquez sur "Démarrer l'enregistrement" pour commencer à enregistrer. L'enregistrement se fait en continu par segments de 2 minutes qui sont automatiquement envoyés pour transcription.</li>
  <li><strong>Transcription :</strong> Dès qu'un segment (ou le dernier segment plus court) est terminé, il est envoyé et transcrit en arrière-plan. Après arrêt, le statut sera mis à jour en "Prêt à transcrire !" et vous pourrez cliquer sur "Transcrire" pour récupérer la transcription compilée.</li>
  <li><strong>Génération de notes :</strong> Après la transcription, cliquez sur "Générer une note" pour créer une note basée sur la transcription et votre message personnalisé.</li>
  <li><strong>Message personnalisé :</strong> Sélectionnez un emplacement (1–10) et saisissez votre message personnalisé. Celui-ci sera sauvegardé automatiquement et associé à votre clé API.</li>
  <li><strong>Guide :</strong> Utilisez "Fonctions" et "Guide" pour alterner entre l'interface et ce guide.</li>
</ul>
Cliquez sur "Fonctions" pour revenir à l'interface principale.`
        }
      };
      
      // Flag to mark that transcription has been fetched.
      let transcriptionFetched = false;
      
      function updateLanguageTranscribe(lang) {
        document.getElementById("page-title-transcribe").textContent = transcribeTranslations[lang].pageTitle;
        document.getElementById("openaiUsageLink").textContent = transcribeTranslations[lang].openaiUsageLinkText;
        document.getElementById("btnFunctions").textContent = transcribeTranslations[lang].btnFunctions;
        document.getElementById("btnGuide").textContent = transcribeTranslations[lang].btnGuide;
        document.getElementById("recordingAreaTitle").textContent = transcribeTranslations[lang].recordingAreaTitle;
        document.getElementById("recordTimer").textContent = transcribeTranslations[lang].recordTimer;
        document.getElementById("uploadTimer").textContent = transcribeTranslations[lang].uploadTimer;
        document.getElementById("transcribeTimer").textContent = transcribeTranslations[lang].transcribeTimer;
        document.getElementById("transcription").setAttribute("placeholder", transcribeTranslations[lang].transcriptionPlaceholder);
        document.getElementById("startButton").textContent = transcribeTranslations[lang].startButton;
        document.getElementById("stopButton").textContent = transcribeTranslations[lang].stopButton;
        document.getElementById("pauseResumeButton").textContent = transcribeTranslations[lang].pauseButton;
        document.getElementById("transcribeButton").textContent = transcribeTranslations[lang].transcribeButton;
        document.getElementById("statusMessage").textContent = transcribeTranslations[lang].statusMessage;
        document.getElementById("noteGenerationTitle").textContent = transcribeTranslations[lang].noteGenerationTitle;
        document.getElementById("generateNoteButton").textContent = transcribeTranslations[lang].generateNoteButton;
        document.getElementById("noteTimer").textContent = transcribeTranslations[lang].noteTimer;
        document.getElementById("generatedNote").setAttribute("placeholder", transcribeTranslations[lang].generatedNotePlaceholder);
        document.getElementById("customPromptTitle").textContent = transcribeTranslations[lang].customPromptTitle;
        document.getElementById("promptSlotLabel").textContent = transcribeTranslations[lang].promptSlotLabel;
        document.getElementById("customPrompt").setAttribute("placeholder", transcribeTranslations[lang].customPromptPlaceholder);
        document.getElementById("adUnit").textContent = transcribeTranslations[lang].adUnitText;
        document.getElementById("guideHeading").textContent = transcribeTranslations[lang].guideHeading;
        document.getElementById("guideText").innerHTML = transcribeTranslations[lang].guideText;
      }
      
      let currentLangTranscribe = localStorage.getItem("siteLanguage") || "en";
      document.getElementById("lang-select-transcribe").value = currentLangTranscribe;
      updateLanguageTranscribe(currentLangTranscribe);
      document.getElementById("lang-select-transcribe").addEventListener("change", function() {
        currentLangTranscribe = this.value;
        localStorage.setItem("siteLanguage", currentLangTranscribe);
        updateLanguageTranscribe(currentLangTranscribe);
      });
      
      /* --------------------------
         Consent Banner & Cookie Management for Transcribe Page
         -------------------------- */
      function setCookieTranscribe(name, value, days) {
        var expires = "";
        if (days) {
          var date = new Date();
          date.setTime(date.getTime() + (days * 24 * 60 * 60 * 1000));
          expires = "; expires=" + date.toUTCString();
        }
        document.cookie = name + "=" + (value || "") + expires + "; path=/";
      }
      function getCookieTranscribe(name) {
        var nameEQ = name + "=";
        var ca = document.cookie.split(';');
        for (var i = 0; i < ca.length; i++) {
          var c = ca[i];
          while (c.charAt(0) === ' ') c = c.substring(1);
          if (c.indexOf(nameEQ) === 0) return c.substring(nameEQ.length);
        }
        return null;
      }
      function loadAdSenseTranscribe() {
        var script = document.createElement('script');
        script.async = true;
        script.src = "https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js";
        document.head.appendChild(script);
        (adsbygoogle = window.adsbygoogle || []).push({});
      }
      document.getElementById("cmp-accept").addEventListener("click", function() {
        setCookieTranscribe("user_consent", "accepted", 365);
        document.getElementById("cmp-banner").style.display = "none";
        loadAdSenseTranscribe();
      });
      document.getElementById("cmp-manage").addEventListener("click", function() {
        alert("Here you can manage your cookie and ad preferences.");
      });
      (function() {
        if (getCookieTranscribe("user_consent") === "accepted") {
          document.getElementById("cmp-banner").style.display = "none";
          loadAdSenseTranscribe();
        }
      })();
      
      /* --------------------------
         Continuous Recording & Chunk Upload Logic
         -------------------------- */
      
      const backendUrl = "https://whisper-dev-backend.fly.dev";
      let mediaRecorder;
      let mediaStream = null;
      let currentChunkNumber = 1;
      let groupId = null;
      let isRecording = false;
      let pendingChunks = 0;
      let recordingStartTime = 0;
      let recordingTimerInterval;
      
      function updateStatusMessage(message, color = "#333") {
        const statusMessage = document.getElementById("statusMessage");
        statusMessage.innerText = message;
        statusMessage.style.color = color;
      }
      
      function formatTime(ms) {
        const totalSec = Math.floor(ms / 1000);
        if (totalSec < 60) {
          return totalSec + " sec";
        } else {
          const minutes = Math.floor(totalSec / 60);
          const seconds = totalSec % 60;
          return minutes + " min" + (seconds > 0 ? " " + seconds + " sec" : "");
        }
      }
      
      function updateRecordingTimer() {
        let elapsed = Date.now() - recordingStartTime;
        document.getElementById("recordTimer").innerText = "Recording Timer: " + formatTime(elapsed);
      }
      
      async function uploadChunk(blob, chunkNumber) {
        pendingChunks++;
        const formData = new FormData();
        formData.append("file", blob, `chunk_${chunkNumber}.mp3`);
        formData.append("group_id", groupId);
        formData.append("chunk_number", chunkNumber);
        // Append API key from sessionStorage
        const apiKey = sessionStorage.getItem("openai_api_key");
        formData.append("api_key", apiKey);
        
        try {
          const response = await fetch(`${backendUrl}/upload`, {
            method: "POST",
            body: formData
          });
          const result = await response.json();
          console.log(`Chunk ${chunkNumber} uploaded, session_id: `, result.session_id);
        } catch (error) {
          console.error("Error uploading chunk:", error);
        }
        pendingChunks--;
        // Only re-enable transcribeButton if recording has stopped,
        // no pending chunks remain, and transcription hasn't been fetched yet.
        if (!isRecording && pendingChunks <= 0 && !transcriptionFetched) {
          document.getElementById("transcribeButton").disabled = false;
          updateStatusMessage("Ready to transcribe!", "green");
        }
      }
      
      function handleDataAvailable(event) {
        if (event.data && event.data.size > 0) {
          uploadChunk(event.data, currentChunkNumber);
          currentChunkNumber++;
        }
      }
      
      document.getElementById("startButton").addEventListener("click", async () => {
        isRecording = true;
        transcriptionFetched = false; // Reset the flag when starting a new recording.
        document.getElementById("transcribeButton").disabled = true;
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          groupId = Date.now().toString();
          currentChunkNumber = 1;
          pendingChunks = 0;
          recordingStartTime = Date.now();
          updateRecordingTimer();
          recordingTimerInterval = setInterval(updateRecordingTimer, 1000);
          
          mediaRecorder = new MediaRecorder(mediaStream);
          mediaRecorder.ondataavailable = handleDataAvailable;
          mediaRecorder.onstart = () => {
            updateStatusMessage("Recording started...", "red");
            document.getElementById("recordIndicator").style.backgroundColor = "red";
          };
          // Start recording continuously with timeslice of 2 minutes (120000 ms)
          mediaRecorder.start(120000);
          
          document.getElementById("startButton").disabled = true;
          document.getElementById("stopButton").disabled = false;
          document.getElementById("pauseResumeButton").disabled = false;
          document.getElementById("pauseResumeButton").innerText = transcribeTranslations[currentLangTranscribe].pauseButton;
        } catch (error) {
          updateStatusMessage("Microphone access error: " + error, "red");
        }
      });
      
      document.getElementById("stopButton").addEventListener("click", () => {
        isRecording = false;
        if (mediaRecorder && mediaRecorder.state !== "inactive") {
          mediaRecorder.stop();
        }
        clearInterval(recordingTimerInterval);
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
          mediaStream = null;
        }
        updateStatusMessage("Recording stopped. Getting ready...", "blue");
        document.getElementById("startButton").disabled = false;
        document.getElementById("stopButton").disabled = true;
        document.getElementById("pauseResumeButton").disabled = true;
        // If there are no pending chunks and transcription hasn't been fetched,
        // enable the transcribe button.
        if (pendingChunks <= 0 && !transcriptionFetched) {
          document.getElementById("transcribeButton").disabled = false;
          updateStatusMessage("Ready to transcribe!", "green");
        }
      });
      
      document.getElementById("pauseResumeButton").addEventListener("click", () => {
        if (!mediaRecorder) return;
        if (mediaRecorder.state === "recording") {
          mediaRecorder.pause();
          clearInterval(recordingTimerInterval);
          document.getElementById("pauseResumeButton").innerText = "Resume Recording";
          updateStatusMessage("Recording paused", "orange");
        } else if (mediaRecorder.state === "paused") {
          mediaRecorder.resume();
          recordingStartTime = Date.now();
          recordingTimerInterval = setInterval(updateRecordingTimer, 1000);
          document.getElementById("pauseResumeButton").innerText = transcribeTranslations[currentLangTranscribe].pauseButton;
          updateStatusMessage("Recording resumed", "green");
        }
      });
      
      document.getElementById("transcribeButton").addEventListener("click", async () => {
        updateStatusMessage("Fetching compiled transcription...", "blue");
        try {
          const response = await fetch(`${backendUrl}/get_compiled_transcription`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ group_id: groupId })
          });
          const result = await response.json();
          if (result.compiled_transcription) {
            document.getElementById("transcription").value = result.compiled_transcription;
            updateStatusMessage("Transcription complete!", "green");
            // Mark that transcription has been fetched and disable the transcribe button.
            transcriptionFetched = true;
            document.getElementById("transcribeButton").disabled = true;
          } else {
            updateStatusMessage("No transcription available.", "red");
          }
        } catch (error) {
          updateStatusMessage("Error fetching transcription: " + error, "red");
        }
      });
      
      /* --------------------------
         Custom Prompt and Note Generation Functionality
         -------------------------- */
      
      const promptSlotSelect = document.getElementById("promptSlot");
      const customPromptTextarea = document.getElementById("customPrompt");
      
      function hashString(str) {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
          const char = str.charCodeAt(i);
          hash = ((hash << 5) - hash) + char;
          hash |= 0;
        }
        return hash.toString();
      }
      
      function getPromptStorageKey(slot) {
        const apiKey = sessionStorage.getItem("openai_api_key") || "";
        const hashedApiKey = hashString(apiKey);
        return "customPrompt_" + hashedApiKey + "_" + slot;
      }
      
      function autoResize(textarea) {
        textarea.style.height = "auto";
        textarea.style.height = textarea.scrollHeight + "px";
      }
      
      function loadPromptForSlot(slot) {
        const key = getPromptStorageKey(slot);
        const storedPrompt = localStorage.getItem(key);
        customPromptTextarea.value = storedPrompt ? storedPrompt : "";
        autoResize(customPromptTextarea);
      }
      
      customPromptTextarea.addEventListener("input", () => {
        const currentSlot = promptSlotSelect.value;
        const key = getPromptStorageKey(currentSlot);
        localStorage.setItem(key, customPromptTextarea.value);
        autoResize(customPromptTextarea);
      });
      
      promptSlotSelect.addEventListener("change", () => {
        loadPromptForSlot(promptSlotSelect.value);
      });
      
      loadPromptForSlot(promptSlotSelect.value);
      
      document.getElementById("generateNoteButton").addEventListener("click", async () => {
        const transcriptionText = document.getElementById("transcription").value.trim();
        if (!transcriptionText) {
          alert("No transcription text available.");
          return;
        }
        
        const promptText = customPromptTextarea.value;
        const generatedNoteField = document.getElementById("generatedNote");
        generatedNoteField.value = "";
        
        const noteStartTime = Date.now();
        const noteTimerElement = document.getElementById("noteTimer");
        noteTimerElement.innerText = "Note Generation Timer: 0 sec";
        const noteTimerInterval = setInterval(() => {
          noteTimerElement.innerText = "Note Generation Timer: " + formatTime(Date.now() - noteStartTime);
        }, 1000);
        
        const apiKey = sessionStorage.getItem("openai_api_key");
        try {
          const response = await fetch("https://api.openai.com/v1/chat/completions", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              "Authorization": "Bearer " + apiKey
            },
            body: JSON.stringify({
              model: "gpt-4-turbo",
              messages: [
                { role: "system", content: promptText },
                { role: "user", content: transcriptionText }
              ],
              temperature: 0.7,
              stream: true
            })
          });
          
          const reader = response.body.getReader();
          const decoder = new TextDecoder("utf-8");
          let done = false;
          
          while (!done) {
            const { value, done: doneReading } = await reader.read();
            done = doneReading;
            const chunkValue = decoder.decode(value);
            const lines = chunkValue.split("\n").filter(line => line.trim() !== "");
            for (const line of lines) {
              if (line.startsWith("data: ")) {
                const jsonStr = line.replace("data: ", "").trim();
                if (jsonStr === "[DONE]") {
                  done = true;
                  break;
                }
                try {
                  const parsed = JSON.parse(jsonStr);
                  const textChunk = parsed.choices[0].delta?.content || "";
                  generatedNoteField.value += textChunk;
                  autoResize(generatedNoteField);
                } catch (err) {
                  console.error("Stream chunk parsing error:", err);
                }
              }
            }
          }
          clearInterval(noteTimerInterval);
          noteTimerElement.innerText = "Text generation completed!";
        } catch (error) {
          clearInterval(noteTimerInterval);
          generatedNoteField.value = "Error generating note: " + error;
          noteTimerElement.innerText = "";
        }
      });
    });
  </script>
</body>
</html>
