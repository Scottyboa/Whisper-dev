<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title id="page-title-transcribe">Transcription Tool with Ads and Guide Overlay</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500&family=Merriweather:wght@300;400&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background-color: #f8f8f8;
      color: #333;
      margin: 0;
      padding: 0;
    }
    /* Grid Layout */
    .grid-container {
      display: grid;
      grid-template-columns: 250px 1fr 250px;
      height: 100vh;
    }
    /* Left Sidebar */
    .sidebar {
      background-color: #e0e0e0;
      padding: 20px;
      display: flex;
      flex-direction: column;
      gap: 20px;
      position: relative;
    }
    .sidebar button {
      font-size: 18px;
      padding: 15px;
    }
    /* Main Content Area */
    .main-content {
      padding: 20px;
      display: flex;
      flex-direction: column;
      gap: 20px;
      overflow-y: auto;
    }
    /* Recording Area (Top Half) */
    .recording-area {
      border-bottom: 1px solid #ddd;
      padding-bottom: 20px;
    }
    .timer {
      font-size: 18px;
      margin-bottom: 10px;
      color: #333;
    }
    /* Auto-resizing Textareas */
    #transcription, #generatedNote, #customPrompt {
      width: 100%;
      padding: 12px;
      border: 1px solid #ddd;
      border-radius: 10px;
      font-size: 16px;
      box-sizing: border-box;
      margin-bottom: 20px;
      font-family: 'Roboto', sans-serif;
      resize: none;
    }
    /* Allow vertical resize for the transcription output field */
    #transcription {
      resize: vertical;
    }
    #transcription, #generatedNote {
      min-height: 150px;
    }
    #customPrompt {
      min-height: 200px;
    }
    button {
      background-color: #5a9;
      color: #fff;
      border: none;
      border-radius: 10px;
      padding: 12px 20px;
      font-size: 16px;
      cursor: pointer;
      transition: background-color 0.3s, transform 0.1s;
      margin: 10px;
    }
    button:hover {
      background-color: #489;
    }
    button:active {
      transform: scale(0.98);
    }
    /* Bottom Half: Two Columns */
    .bottom-half {
      display: flex;
      gap: 20px;
      flex: 1;
    }
    .column {
      flex: 1;
      display: flex;
      flex-direction: column;
      gap: 10px;
    }
    /* Right Sidebar (Ad Area) */
    .ad-sidebar {
      background-color: #e0e0e0;
      padding: 20px;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    #adUnit {
      width: 100%;
      height: 200px;
      background-color: #ddd;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 18px;
      margin-bottom: 10px;
    }
    /* Guide Overlay */
    #guideView {
      display: none;
      position: fixed;
      top: 10%;
      left: 10%;
      width: 80%;
      height: 80%;
      background: white;
      border: 2px solid #ccc;
      padding: 20px;
      overflow-y: auto;
      z-index: 2000;
    }
    #guideView.active {
      display: block;
    }
    /* Language Dropdown in Sidebar */
    #lang-container-transcribe {
      position: absolute;
      bottom: 20px;
      left: 20px;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    #lang-icon-transcribe {
      width: 28px;
      height: 28px;
    }
    /* Consent Banner */
    #cmp-banner {
      position: fixed;
      bottom: 0;
      left: 0;
      right: 0;
      background: #f8f8f8;
      border-top: 1px solid #ccc;
      padding: 20px;
      text-align: center;
      font-size: 16px;
      z-index: 1000;
      box-shadow: 0 -2px 5px rgba(0,0,0,0.1);
    }
    #cmp-banner button {
      margin-left: 10px;
      padding: 8px 16px;
    }
  </style>
</head>
<body>
  <!-- OpenAI usage hyperlink -->
  <a id="openaiUsageLink" href="https://platform.openai.com/usage" target="_blank">Cost usage overview</a>
  
  <div class="grid-container">
    <!-- Left Sidebar -->
    <aside class="sidebar">
      <button id="btnFunctions">Functions</button>
      <button id="btnGuide">Guide</button>
      <!-- Language dropdown at bottom of sidebar -->
      <div id="lang-container-transcribe">
        <img src="language-icon.png" alt="Language Icon" id="lang-icon-transcribe">
        <select id="lang-select-transcribe">
          <option value="en">English</option>
          <option value="no">Norsk</option>
        </select>
      </div>
    </aside>
    <!-- Main Content -->
    <main class="main-content">
      <!-- Top Half: Recording Area -->
      <div class="recording-area">
        <h3 id="recordingAreaTitle">Recording Area</h3>
        <div id="recordIndicator" style="width:20px; height:20px; border-radius:50%; background-color:grey; margin:0 auto 10px;"></div>
        <div id="recordTimer" class="timer">Recording Timer: 0 sec</div>
        <div id="uploadTimer" class="timer">Upload Timer: 0 sec</div>
        <div id="transcribeTimer" class="timer">Transcription Timer: 0 sec</div>
        <textarea id="transcription" placeholder="Transcription result will appear here..."></textarea>
        <div>
          <button id="startButton">Start Recording</button>
          <button id="stopButton" disabled>Stop Recording</button>
          <button id="pauseResumeButton" disabled>Pause Recording</button>
          <button id="transcribeButton" disabled>Transcribe</button>
        </div>
        <div id="statusMessage">Welcome! Click "Start Recording" to begin.</div>
      </div>
      <!-- Bottom Half: Note Generation and Custom Prompt -->
      <div class="bottom-half">
        <div class="column">
          <h3 id="noteGenerationTitle">Note Generation</h3>
          <button id="generateNoteButton">Generate Note</button>
          <div id="noteTimer" class="timer">Note Generation Timer: 0 sec</div>
          <textarea id="generatedNote" readonly placeholder="Generated note will appear here..."></textarea>
        </div>
        <div class="column">
          <h3 id="customPromptTitle">Custom Prompt</h3>
          <label for="promptSlot" id="promptSlotLabel">Prompt Slot:</label>
          <select id="promptSlot">
            <option value="1">1</option>
            <option value="2">2</option>
            <option value="3">3</option>
            <option value="4">4</option>
            <option value="5">5</option>
            <option value="6">6</option>
            <option value="7">7</option>
            <option value="8">8</option>
            <option value="9">9</option>
            <option value="10">10</option>
          </select>
          <textarea id="customPrompt" placeholder="Enter custom prompt here" rows="1"></textarea>
        </div>
      </div>
    </main>
    <!-- Right Sidebar (Ad Area) -->
    <aside class="ad-sidebar">
      <div id="adArea">
        <div id="adUnit">Your Ad Here</div>
      </div>
    </aside>
  </div>
  <!-- Guide Overlay -->
  <div id="guideView">
    <h3 id="guideHeading">Guide & Instructions</h3>
    <p id="guideText">
      Welcome to the Whisper Transcription tool. This application allows medical professionals, therapists, and other practitioners to record and transcribe consultations, as well as generate professional notes using an AI-powered note generator.
      <br><br>
      <strong>How to Use the Functions:</strong>
      <ul>
        <li><strong>Recording:</strong> Click "Start Recording" to begin capturing audio. Use "Stop Recording" to end and upload your recording.</li>
        <li><strong>Transcription:</strong> Once your audio is uploaded, click "Transcribe" to convert speech to text. The transcription will appear in the top area.</li>
        <li><strong>Note Generation:</strong> After transcription, click "Generate Note" to produce a note based on your transcription and custom prompt. A timer shows the duration of note generation, and the generated note appears below.</li>
        <li><strong>Custom Prompt:</strong> On the right, select a prompt slot (1–10) and enter your custom prompt. Your prompt is saved automatically and linked to your API key.</li>
        <li><strong>Guide Toggle:</strong> Use the "Functions" and "Guide" buttons to switch between the functional view and this guide. The guide appears as an overlay and does not disturb the underlying layout.</li>
      </ul>
      Please click "Functions" to return to the main interface.
    </p>
  </div>
  <!-- Consent Banner -->
  <div id="cmp-banner">
    <span id="consent-text">This website is free to use because we rely solely on ad revenue. We use cookies to personalize ads and improve your experience. By clicking "Accept", you consent to the use of cookies.</span>
    <button id="cmp-accept">Accept</button>
    <button id="cmp-manage">Manage</button>
  </div>
  <script>
    // Frontend Keep-Alive Function: Sends a GET request every 10 seconds to keep the backend machine active.
    function startFrontendKeepAlive() {
      setInterval(() => {
        fetch("https://whisper-dev-backend.fly.dev/frontend-keepalive", { method: "GET" })
          .then(response => console.log("Keep-alive response:", response.status))
          .catch(error => console.error("Keep-alive error:", error));
      }, 10000); // Every 10 seconds
    }

    // Start the keep-alive ping immediately on DOM load.
    startFrontendKeepAlive();

    document.addEventListener("DOMContentLoaded", function() {
      /* --------------------------
         Multi-language & Consent Banner Logic
         -------------------------- */
      const transcribeTranslations = {
        en: {
          pageTitle: "Transcription Tool with Ads and Guide Overlay",
          openaiUsageLinkText: "Cost usage overview",
          btnFunctions: "Functions",
          btnGuide: "Guide",
          recordingAreaTitle: "Recording Area",
          recordTimer: "Recording Timer: 0 sec",
          uploadTimer: "Upload Timer: 0 sec",
          transcribeTimer: "Transcription Timer: 0 sec",
          transcriptionPlaceholder: "Transcription result will appear here...",
          startButton: "Start Recording",
          stopButton: "Stop Recording",
          pauseButton: "Pause Recording",
          transcribeButton: "Transcribe",
          statusMessage: "Welcome! Click \"Start Recording\" to begin.",
          noteGenerationTitle: "Note Generation",
          generateNoteButton: "Generate Note",
          noteTimer: "Note Generation Timer: 0 sec",
          generatedNotePlaceholder: "Generated note will appear here...",
          customPromptTitle: "Custom Prompt",
          promptSlotLabel: "Prompt Slot:",
          customPromptPlaceholder: "Enter custom prompt here",
          adUnitText: "Your Ad Here",
          guideHeading: "Guide & Instructions",
          guideText: `Welcome to the Whisper Transcription tool. This application allows medical professionals, therapists, and other practitioners to record and transcribe consultations, as well as generate professional notes using an AI-powered note generator.
<br><br>
<strong>How to Use the Functions:</strong>
<ul>
  <li><strong>Recording:</strong> Click "Start Recording" to begin capturing audio. Use "Stop Recording" to end and upload your recording.</li>
  <li><strong>Transcription:</strong> Once your audio is uploaded, click "Transcribe" to convert speech to text. The transcription will appear in the top area.</li>
  <li><strong>Note Generation:</strong> After transcription, click "Generate Note" to produce a note based on your transcription and custom prompt. A timer shows the duration of note generation, and the generated note appears below.</li>
  <li><strong>Custom Prompt:</strong> On the right, select a prompt slot (1–10) and enter your custom prompt. Your prompt is saved automatically and linked to your API key.</li>
  <li><strong>Guide Toggle:</strong> Use the "Functions" and "Guide" buttons to switch between the functional view and this guide. The guide appears as an overlay and does not disturb the underlying layout.</li>
</ul>
Please click "Functions" to return to the main interface.`
        },
        no: {
          pageTitle: "Transkripsjonsverktøy med annonser og veiledningsoverlegg",
          openaiUsageLinkText: "Vis OpenAI bruk",
          btnFunctions: "Funksjoner",
          btnGuide: "Veiledning",
          recordingAreaTitle: "Opptaksområde",
          recordTimer: "Opptakstimer: 0 sek",
          uploadTimer: "Opplastningstimer: 0 sek",
          transcribeTimer: "Transkripsjonstimer: 0 sek",
          transcriptionPlaceholder: "Transkripsjonsresultatet vises her...",
          startButton: "Start opptak",
          stopButton: "Stopp opptak",
          pauseButton: "Pause opptak",
          transcribeButton: "Transkriber",
          statusMessage: "Velkommen! Klikk 'Start opptak' for å begynne.",
          noteGenerationTitle: "Notatgenerering",
          generateNoteButton: "Generer notat",
          noteTimer: "Notatgenereringstimer: 0 sek",
          generatedNotePlaceholder: "Generert notat vises her...",
          customPromptTitle: "Tilpasset melding",
          promptSlotLabel: "Meldingsplass:",
          customPromptPlaceholder: "Skriv inn tilpasset melding her",
          adUnitText: "Din annonse her",
          guideHeading: "Veiledning og Instruksjoner",
          guideText: `Velkommen til Whisper Transkripsjonsverktøy. Denne applikasjonen lar medisinske fagpersoner, terapeuter og andre utøvere ta opp og transkribere konsultasjoner, samt generere profesjonelle notater ved hjelp av en AI-drevet notatgenerator.
<br><br>
<strong>Slik bruker du verktøyet:</strong>
<ul>
  <li><strong>Opptak:</strong> Klikk på "Start opptak" for å starte opptaket. Klikk på "Stopp opptak" for å avslutte og laste opp opptaket.</li>
  <li><strong>Transkripsjon:</strong> Når opptaket er lastet opp, klikk "Transkriber" for å omforme tale til tekst. Transkripsjonen vises øverst.</li>
  <li><strong>Notatgenerering:</strong> Etter transkripsjonen, klikk "Generer notat" for å lage et notat basert på opptaket og din tilpassede melding. En timer viser varigheten av notatgenereringen, og det genererte notatet vises nedenfor.</li>
  <li><strong>Tilpasset melding:</strong> Velg et meldingsfelt (1–10) og skriv inn din tilpassede melding. Meldingen lagres automatisk og knyttes til din API-nøkkel.</li>
  <li><strong>Veiledning:</strong> Bruk knappene "Funksjoner" og "Veiledning" for å bytte mellom verktøysgrensesnittet og denne veiledningen.</li>
</ul>
Klikk "Funksjoner" for å gå tilbake til hovedskjermen.`
        }
      };
      
      function updateLanguageTranscribe(lang) {
        document.getElementById("page-title-transcribe").textContent = transcribeTranslations[lang].pageTitle;
        document.getElementById("openaiUsageLink").textContent = transcribeTranslations[lang].openaiUsageLinkText;
        document.getElementById("btnFunctions").textContent = transcribeTranslations[lang].btnFunctions;
        document.getElementById("btnGuide").textContent = transcribeTranslations[lang].btnGuide;
        document.getElementById("recordingAreaTitle").textContent = transcribeTranslations[lang].recordingAreaTitle;
        document.getElementById("recordTimer").textContent = transcribeTranslations[lang].recordTimer;
        document.getElementById("uploadTimer").textContent = transcribeTranslations[lang].uploadTimer;
        document.getElementById("transcribeTimer").textContent = transcribeTranslations[lang].transcribeTimer;
        document.getElementById("transcription").setAttribute("placeholder", transcribeTranslations[lang].transcriptionPlaceholder);
        document.getElementById("startButton").textContent = transcribeTranslations[lang].startButton;
        document.getElementById("stopButton").textContent = transcribeTranslations[lang].stopButton;
        document.getElementById("pauseResumeButton").textContent = transcribeTranslations[lang].pauseButton;
        document.getElementById("transcribeButton").textContent = transcribeTranslations[lang].transcribeButton;
        document.getElementById("statusMessage").textContent = transcribeTranslations[lang].statusMessage;
        document.getElementById("noteGenerationTitle").textContent = transcribeTranslations[lang].noteGenerationTitle;
        document.getElementById("generateNoteButton").textContent = transcribeTranslations[lang].generateNoteButton;
        document.getElementById("noteTimer").textContent = transcribeTranslations[lang].noteTimer;
        document.getElementById("generatedNote").setAttribute("placeholder", transcribeTranslations[lang].generatedNotePlaceholder);
        document.getElementById("customPromptTitle").textContent = transcribeTranslations[lang].customPromptTitle;
        document.getElementById("promptSlotLabel").textContent = transcribeTranslations[lang].promptSlotLabel;
        document.getElementById("customPrompt").setAttribute("placeholder", transcribeTranslations[lang].customPromptPlaceholder);
        document.getElementById("adUnit").textContent = transcribeTranslations[lang].adUnitText;
        document.getElementById("guideHeading").textContent = transcribeTranslations[lang].guideHeading;
        document.getElementById("guideText").innerHTML = transcribeTranslations[lang].guideText;
      }
      let currentLangTranscribe = localStorage.getItem("siteLanguage") || "en";
      document.getElementById("lang-select-transcribe").value = currentLangTranscribe;
      updateLanguageTranscribe(currentLangTranscribe);
      document.getElementById("lang-select-transcribe").addEventListener("change", function() {
        currentLangTranscribe = this.value;
        localStorage.setItem("siteLanguage", currentLangTranscribe);
        updateLanguageTranscribe(currentLangTranscribe);
      });
      
      /* --------------------------
         Consent Banner & Cookie Management for Transcribe Page
         -------------------------- */
      function setCookieTranscribe(name, value, days) {
        var expires = "";
        if (days) {
          var date = new Date();
          date.setTime(date.getTime() + (days * 24 * 60 * 60 * 1000));
          expires = "; expires=" + date.toUTCString();
        }
        document.cookie = name + "=" + (value || "") + expires + "; path=/";
      }
      function getCookieTranscribe(name) {
        var nameEQ = name + "=";
        var ca = document.cookie.split(';');
        for (var i = 0; i < ca.length; i++) {
          var c = ca[i];
          while (c.charAt(0) === ' ') c = c.substring(1);
          if (c.indexOf(nameEQ) === 0) return c.substring(nameEQ.length);
        }
        return null;
      }
      function loadAdSenseTranscribe() {
        var script = document.createElement('script');
        script.async = true;
        script.src = "https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js";
        document.head.appendChild(script);
        (adsbygoogle = window.adsbygoogle || []).push({});
      }
      document.getElementById("cmp-accept").addEventListener("click", function() {
        setCookieTranscribe("user_consent", "accepted", 365);
        document.getElementById("cmp-banner").style.display = "none";
        loadAdSenseTranscribe();
      });
      document.getElementById("cmp-manage").addEventListener("click", function() {
        alert("Here you can manage your cookie and ad preferences.");
      });
      (function() {
        if (getCookieTranscribe("user_consent") === "accepted") {
          document.getElementById("cmp-banner").style.display = "none";
          loadAdSenseTranscribe();
        }
      })();
      
      /* --------------------------
         Dual-Recorder (Ping-Pong) Continuous Recording Logic & Transcription Functionality
         -------------------------- */
      
      // Global variables for recording
      let mediaStream = null;
      let recordingStartTime = 0;
      let recordingTimerInterval;
      let groupId = null;
      let chunkNumber = 1;
      let manualStop = false;  // true if user manually stops the overall recording
      let activeRecorder = null;
      
      // Update status message
      function updateStatusMessage(message, color = "#333") {
        const statusMessage = document.getElementById("statusMessage");
        statusMessage.innerText = message;
        statusMessage.style.color = color;
      }
      
      // Format time utility
      function formatTime(ms) {
        const totalSec = Math.floor(ms / 1000);
        if (totalSec < 60) {
          return totalSec + " sec";
        } else {
          const minutes = Math.floor(totalSec / 60);
          const seconds = totalSec % 60;
          return minutes + " min" + (seconds > 0 ? " " + seconds + " sec" : "");
        }
      }
      
      // Update recording timer display
      function updateRecordingTimer() {
        let elapsed = Date.now() - recordingStartTime;
        document.getElementById("recordTimer").innerText = "Recording Timer: " + formatTime(elapsed);
      }
      
      // Stop microphone
      function stopMicrophone() {
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
          mediaStream = null;
        }
      }
      
      // Upload a given chunk with an optional last chunk marker
      async function uploadChunk(blob, currentChunkNumber, extension, mimeType, isLast = false) {
        const formData = new FormData();
        formData.append("file", blob, `chunk_${currentChunkNumber}.${extension}`);
        formData.append("group_id", groupId);
        formData.append("chunk_number", currentChunkNumber);
        // Append the API key from session storage.
        formData.append("api_key", sessionStorage.getItem("openai_api_key"));
        if (isLast) {
          formData.append("last_chunk", "true");
        }
        
        // Start the upload timer
        let uploadStartTime = Date.now();
        document.getElementById("uploadTimer").innerText = "Upload Timer: 0 sec";
        let uploadTimerInterval = setInterval(() => {
          let elapsed = Date.now() - uploadStartTime;
          document.getElementById("uploadTimer").innerText = "Upload Timer: " + formatTime(elapsed);
        }, 1000);
        
        try {
          const response = await fetch(`${backendUrl}/upload`, {
            method: "POST",
            body: formData
          });
          const result = await response.json();
          clearInterval(uploadTimerInterval);
          if (result.session_id) {
            console.log(`Chunk ${currentChunkNumber} uploaded. Session ID: ${result.session_id}`);
          } else {
            updateStatusMessage("Upload error on chunk " + currentChunkNumber + ": " + (result.error || "Unknown error"), "red");
          }
        } catch (error) {
          clearInterval(uploadTimerInterval);
          updateStatusMessage("Error uploading chunk " + currentChunkNumber + ": " + error, "red");
        }
      }
      
      // Start ping-pong recording: each chunk is recorded by a fresh recorder instance.
      function startPingPongRecording() {
        recordingStartTime = Date.now();
        clearInterval(recordingTimerInterval);
        updateRecordingTimer();
        recordingTimerInterval = setInterval(updateRecordingTimer, 1000);
        startNewRecorder();
        updateStatusMessage("Recording started...", "red");
        document.getElementById("recordIndicator").style.backgroundColor = "red";
      }
      
      // Start a new recorder instance and schedule its stop after 2 minutes.
      function startNewRecorder() {
        activeRecorder = new MediaRecorder(mediaStream);
        activeRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            let actualMimeType = activeRecorder.mimeType || 'audio/mp3';
            let extension = 'mp3';
            if(actualMimeType.indexOf("webm") !== -1) {
              extension = "webm";
            } else if(actualMimeType.indexOf("ogg") !== -1) {
              extension = "ogg";
            }
            let audioBlob = new Blob([event.data], { type: actualMimeType });
            updateStatusMessage(`Uploading chunk ${chunkNumber}...`, "blue");
            // If manualStop is true, mark this chunk as the last chunk.
            uploadChunk(audioBlob, chunkNumber, extension, actualMimeType, manualStop)
              .then(() => {
                console.log(`Chunk ${chunkNumber} uploaded.`);
              })
              .catch((err) => {
                console.error(`Upload error for chunk ${chunkNumber}:`, err);
              });
            chunkNumber++;
          }
          if (!manualStop) {
            startNewRecorder();
          } else {
            updateStatusMessage("Upload complete! You may now transcribe.", "green");
            document.getElementById("transcribeButton").disabled = false;
          }
        };
        activeRecorder.start();
        // Schedule stopping this recorder after 2 minutes (120000 ms)
        setTimeout(() => {
          if (activeRecorder && activeRecorder.state === "recording") {
            activeRecorder.stop();
          }
        }, 120000);
      }
      
      // Define the backend URL
      const backendUrl = "https://whisper-dev-backend.fly.dev";
      
      // Start Button Event: Initialize group and start ping-pong recording
      document.getElementById("startButton").addEventListener("click", async () => {
        manualStop = false;
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          groupId = Date.now().toString();
          chunkNumber = 1;
          startPingPongRecording();
      
          document.getElementById("startButton").disabled = true;
          document.getElementById("stopButton").disabled = false;
          document.getElementById("pauseResumeButton").disabled = false;
          document.getElementById("pauseResumeButton").innerText = "Pause Recording";
        } catch (error) {
          updateStatusMessage("Microphone access error: " + error, "red");
        }
      });
      
      // Stop Button Event: User manually stops the overall recording
      document.getElementById("stopButton").addEventListener("click", () => {
        manualStop = true;
        if (activeRecorder && activeRecorder.state !== "inactive") {
          activeRecorder.stop();
          clearInterval(recordingTimerInterval);
        }
        document.getElementById("startButton").disabled = false;
        document.getElementById("stopButton").disabled = true;
        document.getElementById("pauseResumeButton").disabled = true;
      });
      
      // Pause/Resume Button Event
      document.getElementById("pauseResumeButton").addEventListener("click", () => {
        if (!activeRecorder) return;
        if (activeRecorder.state === "recording") {
          activeRecorder.pause();
          clearInterval(recordingTimerInterval);
          document.getElementById("pauseResumeButton").innerText = "Resume Recording";
          updateStatusMessage("Recording paused", "orange");
        } else if (activeRecorder.state === "paused") {
          activeRecorder.resume();
          recordingStartTime = Date.now();
          recordingTimerInterval = setInterval(updateRecordingTimer, 1000);
          document.getElementById("pauseResumeButton").innerText = "Pause Recording";
          updateStatusMessage("Recording resumed", "green");
        }
      });
      
      // Transcribe Button Event
      document.getElementById("transcribeButton").addEventListener("click", async () => {
        const storedApiKey = sessionStorage.getItem("openai_api_key");
        if (!storedApiKey) {
          updateStatusMessage("API key missing.", "red");
          return;
        }
      
        updateStatusMessage("Transcription in progress...", "blue");
        const transcriptionStart = Date.now();
        const transcriptionTimerInterval = setInterval(() => {
          document.getElementById("transcribeTimer").innerText = "Transcription Timer: " + Math.floor((Date.now() - transcriptionStart) / 1000) + " s";
        }, 1000);
      
        try {
          const response = await fetch(`${backendUrl}/transcribe`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ session_id: groupId, api_key: storedApiKey })
          });
          const result = await response.json();
          clearInterval(transcriptionTimerInterval);
      
          if (result.transcription) {
            updateStatusMessage("Transcription complete!", "green");
            document.getElementById("transcription").value = result.transcription;
          } else {
            updateStatusMessage("No transcription available.", "red");
          }
        } catch (error) {
          updateStatusMessage("Transcription error: " + error, "red");
        }
      });
      
      /* --------------------------
         Custom Prompt and Note Generation Functionality
         -------------------------- */
      
      // Custom Prompt Section: load and save prompts locally linked with API key
      const promptSlotSelect = document.getElementById("promptSlot");
      const customPromptTextarea = document.getElementById("customPrompt");
      
      function hashString(str) {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
          const char = str.charCodeAt(i);
          hash = ((hash << 5) - hash) + char;
          hash |= 0;
        }
        return hash.toString();
      }
      
      function getPromptStorageKey(slot) {
        const apiKey = sessionStorage.getItem("openai_api_key") || "";
        const hashedApiKey = hashString(apiKey);
        return "customPrompt_" + hashedApiKey + "_" + slot;
      }
      
      function autoResize(textarea) {
        textarea.style.height = "auto";
        textarea.style.height = textarea.scrollHeight + "px";
      }
      
      function loadPromptForSlot(slot) {
        const key = getPromptStorageKey(slot);
        const storedPrompt = localStorage.getItem(key);
        customPromptTextarea.value = storedPrompt ? storedPrompt : "";
        autoResize(customPromptTextarea);
      }
      
      customPromptTextarea.addEventListener("input", () => {
        const currentSlot = promptSlotSelect.value;
        const key = getPromptStorageKey(currentSlot);
        localStorage.setItem(key, customPromptTextarea.value);
        autoResize(customPromptTextarea);
      });
      
      promptSlotSelect.addEventListener("change", () => {
        loadPromptForSlot(promptSlotSelect.value);
      });
      
      // Initially load prompt for the default slot
      loadPromptForSlot(promptSlotSelect.value);
      
      // Note Generation Event Listener
      document.getElementById("generateNoteButton").addEventListener("click", async () => {
        const transcriptionText = document.getElementById("transcription").value.trim();
        if (!transcriptionText) {
          alert("No transcription text available.");
          return;
        }
        
        const promptText = customPromptTextarea.value;
        const generatedNoteField = document.getElementById("generatedNote");
        generatedNoteField.value = "";
        
        const noteStartTime = Date.now();
        const noteTimerElement = document.getElementById("noteTimer");
        noteTimerElement.innerText = "Note Generation Timer: 0 sec";
        const noteTimerInterval = setInterval(() => {
          noteTimerElement.innerText = "Note Generation Timer: " + formatTime(Date.now() - noteStartTime);
        }, 1000);
        
        const apiKey = sessionStorage.getItem("openai_api_key");
        try {
          const response = await fetch("https://api.openai.com/v1/chat/completions", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              "Authorization": "Bearer " + apiKey
            },
            body: JSON.stringify({
              model: "gpt-4-turbo",
              messages: [
                { role: "system", content: promptText },
                { role: "user", content: transcriptionText }
              ],
              temperature: 0.7,
              stream: true
            })
          });
          
          const reader = response.body.getReader();
          const decoder = new TextDecoder("utf-8");
          let done = false;
          
          while (!done) {
            const { value, done: doneReading } = await reader.read();
            done = doneReading;
            const chunkValue = decoder.decode(value);
            const lines = chunkValue.split("\n").filter(line => line.trim() !== "");
            for (const line of lines) {
              if (line.startsWith("data: ")) {
                const jsonStr = line.replace("data: ", "").trim();
                if (jsonStr === "[DONE]") {
                  done = true;
                  break;
                }
                try {
                  const parsed = JSON.parse(jsonStr);
                  const textChunk = parsed.choices[0].delta?.content || "";
                  generatedNoteField.value += textChunk;
                  autoResize(generatedNoteField);
                } catch (err) {
                  console.error("Stream chunk parsing error:", err);
                }
              }
            }
          }
          clearInterval(noteTimerInterval);
          noteTimerElement.innerText = "Text generation completed!";
        } catch (error) {
          clearInterval(noteTimerInterval);
          generatedNoteField.value = "Error generating note: " + error;
          noteTimerElement.innerText = "";
        }
      });
      
      // ---------------- End of Custom Prompt & Note Generation ----------------
    });
  </script>
</body>
</html>
