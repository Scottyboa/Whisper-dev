<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transcription Tool with Ads and Guide Overlay</title>
  <!-- Bootswatch Cosmo Theme (Bootstrap 5) -->
  <link href="https://cdn.jsdelivr.net/npm/bootswatch@5.3.0/dist/cosmo/bootstrap.min.css" rel="stylesheet" />
  <style>
    /* Custom Styles on Top of Bootstrap */
    
    /* Position the Cost Usage Overview link near the top right */
    #openaiUsageLink {
      position: absolute;
      top: 10px;
      right: 20px;
      font-size: 20px;
      text-decoration: none;
    }
    
    /* Recording indicator animation */
    @keyframes pulseRed {
      0% { background-color: #ff4d4d; }
      50% { background-color: #cc0000; }
      100% { background-color: #ff4d4d; }
    }
    .recording {
      animation: pulseRed 1s infinite;
    }
    
    /* Optional: Override Bootstrap disabled button appearance if needed */
    button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }
    
    /* Sidebar custom styling */
    .sidebar, .ad-sidebar {
      background-color: #e9ecef;
      padding: 1rem;
      border-radius: 0.25rem;
    }
    
    /* Recording area styling */
    .recording-area {
      padding: 1rem;
      border: 1px solid #dee2e6;
      border-radius: 0.25rem;
      margin-bottom: 1rem;
    }
    
    /* Status message styling */
    .status-message {
      margin-top: 1rem;
      font-size: 1.2rem;
    }
  </style>
</head>
<body class="position-relative">
  <!-- Cost Usage Overview Hyperlink -->
  <a id="openaiUsageLink" href="https://platform.openai.com/usage" target="_blank" class="btn btn-outline-secondary">
    Cost usage overview
  </a>

  <div class="container-fluid mt-5">
    <div class="row">
      <!-- Left Sidebar -->
      <div class="col-md-3 mb-3">
        <div class="sidebar">
          <button id="btnFunctions" class="btn btn-primary w-100 mb-3">Functions</button>
          <button id="btnGuide" class="btn btn-primary w-100">Guide</button>
          <div id="lang-container-transcribe" class="mt-3">
            <img src="language-icon.png" alt="Language Icon" id="lang-icon-transcribe" style="width:28px; height:28px;" />
            <select id="lang-select-transcribe" class="form-select d-inline-block w-auto ms-2">
              <option value="en">English</option>
              <option value="no">Norsk</option>
            </select>
          </div>
        </div>
      </div>
      <!-- Main Content -->
      <div class="col-md-6 mb-3">
        <div class="recording-area">
          <h3 id="recordingAreaTitle">Recording Area</h3>
          <div id="recordIndicator" style="width:20px; height:20px; border-radius:50%; background-color:grey; margin:0 auto 10px;"></div>
          <div id="recordTimer" class="timer">Recording Timer: 0 sec</div>
          <div id="transcribeTimer" class="timer">Completion Timer: 0 sec</div>
          <textarea id="transcription" class="form-control mb-3" placeholder="Transcription result will appear here..."></textarea>
          <div class="d-flex justify-content-between">
            <button id="startButton" class="btn btn-success">Start Recording</button>
            <button id="stopButton" class="btn btn-danger" disabled>Stop/Complete</button>
            <button id="pauseResumeButton" class="btn btn-warning" disabled>Pause Recording</button>
          </div>
          <div id="statusMessage" class="status-message">Welcome! Click "Start Recording" to begin.</div>
        </div>
        <div class="row">
          <div class="col-md-6 mb-3">
            <h3 id="noteGenerationTitle">Note Generation</h3>
            <button id="generateNoteButton" class="btn btn-primary mb-3">Generate Note</button>
            <div id="noteTimer" class="timer">Note Generation Timer: 0 sec</div>
            <textarea id="generatedNote" class="form-control" readonly placeholder="Generated note will appear here..."></textarea>
          </div>
          <div class="col-md-6 mb-3">
            <h3 id="customPromptTitle">Custom Prompt</h3>
            <label for="promptSlot" id="promptSlotLabel">Prompt Slot:</label>
            <select id="promptSlot" class="form-select mb-3 d-inline-block w-auto">
              <option value="1">1</option>
              <option value="2">2</option>
              <option value="3">3</option>
              <option value="4">4</option>
              <option value="5">5</option>
              <option value="6">6</option>
              <option value="7">7</option>
              <option value="8">8</option>
              <option value="9">9</option>
              <option value="10">10</option>
            </select>
            <textarea id="customPrompt" class="form-control" placeholder="Enter custom prompt here" rows="3"></textarea>
          </div>
        </div>
      </div>
      <!-- Right Sidebar (Ad Area) -->
      <div class="col-md-3 mb-3">
        <div class="ad-sidebar">
          <div id="adArea">
            <div id="adUnit" class="d-flex align-items-center justify-content-center" style="height:200px;">
              Your Ad Here
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Guide Overlay (Using Bootstrap Modal) -->
  <div id="guideView" class="modal" tabindex="-1" style="display:none;">
    <div class="modal-dialog modal-lg">
      <div class="modal-content">
        <div class="modal-header">
          <h3 id="guideHeading" class="modal-title">Guide & Instructions</h3>
          <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
        </div>
        <div class="modal-body">
          <p id="guideText">
            Welcome to the Whisper Transcription tool. This application allows medical professionals, therapists, and other practitioners to record and transcribe consultations, as well as generate professional notes using an AI-powered note generator.
            <br /><br />
            <strong>How to Use the Functions:</strong>
            <ul>
              <li><strong>Recording:</strong> Click "Start Recording" to begin capturing audio. Audio is captured via MediaStreamTrackProcessor (using WebCodecs) and accumulated for up to 40 seconds before being packaged as a self-contained WAV file. A 0.5‑second overlap between chunks is maintained.</li>
              <li><strong>Completion:</strong> After clicking "Stop/Complete", the recording stops. A 2‑second final capture period collects any remaining audio before processing the final chunk. The Completion Timer then ticks until the full transcript is received.</li>
              <li><strong>Note Generation:</strong> After transcription, click "Generate Note" to produce a note based on your transcript and custom prompt.</li>
              <li><strong>Custom Prompt:</strong> On the right, select a prompt slot (1–10) and enter your custom prompt. Your prompt is saved automatically and linked to your API key.</li>
              <li><strong>Guide Toggle:</strong> Use the "Functions" and "Guide" buttons to switch between the functional view and this guide.</li>
            </ul>
            Please click "Functions" to return to the main interface.
          </p>
        </div>
      </div>
    </div>
  </div>

  <!-- Consent Banner -->
  <div id="cmp-banner" class="alert alert-secondary fixed-bottom mb-0 text-center" role="alert">
    <span id="consent-text">
      This website is free to use because we rely solely on ad revenue. We use cookies to personalize ads and improve your experience. By clicking "Accept", you consent to the use of cookies.
    </span>
    <button id="cmp-accept" class="btn btn-primary ms-2">Accept</button>
    <button id="cmp-manage" class="btn btn-outline-primary ms-2">Manage</button>
  </div>

  <!-- Bootstrap Bundle with Popper -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      /* --------------------------
         Multi-language & Consent Banner Logic
         -------------------------- */
      const transcribeTranslations = {
        en: {
          pageTitle: "Transcription Tool with Ads and Guide Overlay",
          openaiUsageLinkText: "Cost usage overview",
          btnFunctions: "Functions",
          btnGuide: "Guide",
          recordingAreaTitle: "Recording Area",
          recordTimer: "Recording Timer: 0 sec",
          transcribeTimer: "Completion Timer: 0 sec",
          transcriptionPlaceholder: "Transcription result will appear here...",
          startButton: "Start Recording",
          stopButton: "Stop/Complete",
          pauseButton: "Pause Recording",
          statusMessage: "Welcome! Click \"Start Recording\" to begin.",
          noteGenerationTitle: "Note Generation",
          generateNoteButton: "Generate Note",
          noteTimer: "Note Generation Timer: 0 sec",
          generatedNotePlaceholder: "Generated note will appear here...",
          customPromptTitle: "Custom Prompt",
          promptSlotLabel: "Prompt Slot:",
          customPromptPlaceholder: "Enter custom prompt here",
          adUnitText: "Your Ad Here",
          guideHeading: "Guide & Instructions",
          guideText: `Welcome to the Whisper Transcription tool. This application allows medical professionals, therapists, and other practitioners to record and transcribe consultations, as well as generate professional notes using an AI-powered note generator.
<br><br>
<strong>How to Use the Functions:</strong>
<ul>
  <li><strong>Recording:</strong> Click "Start Recording" to begin capturing audio. Audio is captured via MediaStreamTrackProcessor (using WebCodecs) and accumulated for up to 40 seconds before being packaged as a self-contained WAV file. A 0.5‑second overlap between chunks is maintained.</li>
  <li><strong>Completion:</strong> After clicking "Stop/Complete", the recording stops. A 2‑second final capture period collects any remaining audio before processing the final chunk. The Completion Timer then ticks until the full transcript is received.</li>
  <li><strong>Note Generation:</strong> After transcription, click "Generate Note" to produce a note based on your transcript and custom prompt.</li>
  <li><strong>Custom Prompt:</strong> On the right, select a prompt slot (1–10) and enter your custom prompt. Your prompt is saved automatically and linked to your API key.</li>
  <li><strong>Guide Toggle:</strong> Use the "Functions" and "Guide" buttons to switch between the functional view and this guide.</li>
</ul>
Please click "Functions" to return to the main interface.`
        },
        no: {
          pageTitle: "Transkripsjonsverktøy med annonser og veiledningsoverlegg",
          openaiUsageLinkText: "Vis OpenAI bruk",
          btnFunctions: "Funksjoner",
          btnGuide: "Veiledning",
          recordingAreaTitle: "Opptaksområde",
          recordTimer: "Opptakstimer: 0 sek",
          transcribeTimer: "Fullføringstimer: 0 sek",
          transcriptionPlaceholder: "Transkripsjonsresultatet vises her...",
          startButton: "Start opptak",
          stopButton: "Stopp/Fullfør",
          pauseButton: "Pause opptak",
          statusMessage: "Velkommen! Klikk 'Start opptak' for å begynne.",
          noteGenerationTitle: "Notatgenerering",
          generateNoteButton: "Generer notat",
          noteTimer: "Notatgenereringstimer: 0 sek",
          generatedNotePlaceholder: "Generert notat vises her...",
          customPromptTitle: "Tilpasset melding",
          promptSlotLabel: "Meldingsplass:",
          customPromptPlaceholder: "Skriv inn tilpasset melding her",
          adUnitText: "Din annonse her",
          guideHeading: "Veiledning og Instruksjoner",
          guideText: `Velkommen til Whisper Transkripsjonsverktøy. Denne applikasjonen lar medisinske fagpersoner, terapeuter og andre utøvere ta opp og transkribere konsultasjoner, samt generere profesjonelle notater ved hjelp av en AI-drevet notatgenerator.
<br><br>
<strong>Slik bruker du verktøyet:</strong>
<ul>
  <li><strong>Opptak:</strong> Klikk på "Start opptak" for å starte opptaket. Lydopptak fanges via MediaStreamTrackProcessor (med WebCodecs) og akkumuleres i opptil 40 sekunders biter før de pakkes som gyldige WAV-filer. Det beholdes en 0,5-sekunders overlapping mellom bitene.</li>
  <li><strong>Fullføring:</strong> Etter at du klikker "Stopp/Fullfør", avsluttes opptaket. En 2-sekunders sluttfase fanger opp resterende lyd, og deretter tickes fullføringstimeren til transkripsjonen er ferdig.</li>
  <li><strong>Notatgenerering:</strong> Etter transkripsjonen, klikk "Generer notat" for å lage et notat basert på transkripsjonen og din tilpassede melding.</li>
  <li><strong>Tilpasset melding:</strong> Velg et meldingsfelt (1–10) og skriv inn din tilpassede melding. Meldingen lagres automatisk og knyttes til din API-nøkkel.</li>
  <li><strong>Veiledning:</strong> Bruk knappene "Funksjoner" og "Veiledning" for å bytte mellom verktøysgrensesnittet og denne veiledningen.</li>
</ul>
Klikk "Funksjoner" for å gå tilbake til hovedskjermen.`
        }
      };
      
      function updateLanguageTranscribe(lang) {
        document.getElementById("page-title-transcribe").textContent = transcribeTranslations[lang].pageTitle;
        document.getElementById("openaiUsageLink").textContent = transcribeTranslations[lang].openaiUsageLinkText;
        document.getElementById("btnFunctions").textContent = transcribeTranslations[lang].btnFunctions;
        document.getElementById("btnGuide").textContent = transcribeTranslations[lang].btnGuide;
        document.getElementById("recordingAreaTitle").textContent = transcribeTranslations[lang].recordingAreaTitle;
        document.getElementById("recordTimer").textContent = transcribeTranslations[lang].recordTimer;
        document.getElementById("transcribeTimer").textContent = transcribeTranslations[lang].transcribeTimer;
        document.getElementById("transcription").setAttribute("placeholder", transcribeTranslations[lang].transcriptionPlaceholder);
        document.getElementById("startButton").textContent = transcribeTranslations[lang].startButton;
        document.getElementById("stopButton").textContent = transcribeTranslations[lang].stopButton;
        document.getElementById("pauseResumeButton").textContent = transcribeTranslations[lang].pauseButton;
        document.getElementById("statusMessage").textContent = transcribeTranslations[lang].statusMessage;
        document.getElementById("noteGenerationTitle").textContent = transcribeTranslations[lang].noteGenerationTitle;
        document.getElementById("generateNoteButton").textContent = transcribeTranslations[lang].generateNoteButton;
        document.getElementById("noteTimer").textContent = transcribeTranslations[lang].noteTimer;
        document.getElementById("generatedNote").setAttribute("placeholder", transcribeTranslations[lang].generatedNotePlaceholder);
        document.getElementById("customPromptTitle").textContent = transcribeTranslations[lang].customPromptTitle;
        document.getElementById("promptSlotLabel").textContent = transcribeTranslations[lang].promptSlotLabel;
        document.getElementById("customPrompt").setAttribute("placeholder", transcribeTranslations[lang].customPromptPlaceholder);
        document.getElementById("adUnit").textContent = transcribeTranslations[lang].adUnitText;
        document.getElementById("guideHeading").textContent = transcribeTranslations[lang].guideHeading;
        document.getElementById("guideText").innerHTML = transcribeTranslations[lang].guideText;
      }
      
      let currentLangTranscribe = localStorage.getItem("siteLanguage") || "en";
      document.getElementById("lang-select-transcribe").value = currentLangTranscribe;
      updateLanguageTranscribe(currentLangTranscribe);
      document.getElementById("lang-select-transcribe").addEventListener("change", function() {
        currentLangTranscribe = this.value;
        localStorage.setItem("siteLanguage", currentLangTranscribe);
        updateLanguageTranscribe(currentLangTranscribe);
      });
      
      /* --------------------------
         Consent Banner & Cookie Management for Transcribe Page
         -------------------------- */
      function setCookieTranscribe(name, value, days) {
        var expires = "";
        if (days) {
          var date = new Date();
          date.setTime(date.getTime() + (days * 24 * 60 * 60 * 1000));
          expires = "; expires=" + date.toUTCString();
        }
        document.cookie = name + "=" + (value || "") + expires + "; path=/";
      }
      function getCookieTranscribe(name) {
        var nameEQ = name + "=";
        var ca = document.cookie.split(';');
        for (var i = 0; i < ca.length; i++) {
          var c = ca[i];
          while (c.charAt(0) === ' ') c = c.substring(1);
          if (c.indexOf(nameEQ) === 0) return c.substring(nameEQ.length);
        }
        return null;
      }
      function loadAdSenseTranscribe() {
        var script = document.createElement('script');
        script.async = true;
        script.src = "https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js";
        document.head.appendChild(script);
        (adsbygoogle = window.adsbygoogle || []).push({});
      }
      document.getElementById("cmp-accept").addEventListener("click", function() {
        setCookieTranscribe("user_consent", "accepted", 365);
        document.getElementById("cmp-banner").style.display = "none";
        loadAdSenseTranscribe();
      });
      document.getElementById("cmp-manage").addEventListener("click", function() {
        alert("Here you can manage your cookie and ad preferences.");
      });
      (function() {
        if (getCookieTranscribe("user_consent") === "accepted") {
          document.getElementById("cmp-banner").style.display = "none";
          loadAdSenseTranscribe();
        }
      })();
      
      /* --------------------------
         Recording & Polling Logic Using MediaStreamTrackProcessor & WebCodecs
         -------------------------- */
      let mediaStream = null;
      let audioReader = null; // Global reader variable
      let recordingStartTime = 0;
      let recordingTimerInterval;
      let completionTimerInterval = null;
      let completionStartTime = 0;
      let groupId = null;
      let chunkNumber = 1;
      let manualStop = false;
      let transcriptChunks = {};
      let pollingIntervals = {};
      
      // Dynamic scheduling constants: aim for chunks between 30 and 40 seconds
      const MIN_CHUNK_DURATION = 30000; // 30 seconds
      const MAX_CHUNK_DURATION = 40000; // 40 seconds
      const watchdogThreshold = 1500;    // 1.5 sec with no frame
      let chunkStartTime = 0;
      let lastFrameTime = 0;
      let chunkTimeoutId;
      
      function updateStatusMessage(message, color = "#333") {
        const statusMessage = document.getElementById("statusMessage");
        statusMessage.innerText = message;
        statusMessage.style.color = color;
      }
      
      function formatTime(ms) {
        const totalSec = Math.floor(ms / 1000);
        if (totalSec < 60) {
          return totalSec + " sec";
        } else {
          const minutes = Math.floor(totalSec / 60);
          const seconds = totalSec % 60;
          return minutes + " min" + (seconds > 0 ? " " + seconds + " sec" : "");
        }
      }
      
      function updateRecordingTimer() {
        let elapsed = Date.now() - recordingStartTime;
        document.getElementById("recordTimer").innerText = "Recording Timer: " + formatTime(elapsed);
      }
      
      function stopMicrophone() {
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
          mediaStream = null;
        }
        if (audioReader) {
          audioReader.cancel();
          audioReader = null;
        }
      }
      
      // WAV encoding functions
      function floatTo16BitPCM(input) {
        let output = new Int16Array(input.length);
        for (let i = 0; i < input.length; i++) {
          let s = Math.max(-1, Math.min(1, input[i]));
          output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        return output;
      }
      
      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }
      
      function encodeWAV(samples, sampleRate, numChannels) {
        const buffer = new ArrayBuffer(44 + samples.length * 2);
        const view = new DataView(buffer);
        writeString(view, 0, 'RIFF');
        view.setUint32(4, 36 + samples.length * 2, true);
        writeString(view, 8, 'WAVE');
        writeString(view, 12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * 2, true);
        view.setUint16(32, numChannels * 2, true);
        view.setUint16(34, 16, true);
        writeString(view, 36, 'data');
        view.setUint32(40, samples.length * 2, true);
        let offset = 44;
        for (let i = 0; i < samples.length; i++, offset += 2) {
          view.setInt16(offset, samples[i], true);
        }
        return new Blob([view], { type: 'audio/wav' });
      }
      
      // Process accumulated audio frames into a WAV blob and upload.
      // Implements a 0.5-second overlap.
      async function processAudioChunk() {
        if (audioFrames.length === 0) return;
        let totalOverlap = 0;
        let overlapIndex = audioFrames.length;
        for (let i = audioFrames.length - 1; i >= 0; i--) {
          const duration = audioFrames[i].numberOfFrames / audioFrames[i].sampleRate;
          totalOverlap += duration;
          if (totalOverlap >= 0.5) { // 0.5 sec overlap
            overlapIndex = i;
            break;
          }
        }
        let framesToProcess = audioFrames.slice(0, overlapIndex);
        let framesToKeep = audioFrames.slice(overlapIndex);
        if (framesToProcess.length === 0) {
          return;
        }
        const sampleRate = framesToProcess[0].sampleRate;
        const numChannels = framesToProcess[0].numberOfChannels;
        let pcmDataArray = [];
        for (let frame of framesToProcess) {
          const numFrames = frame.numberOfFrames;
          if (numChannels === 1) {
            let channelData = new Float32Array(numFrames);
            frame.copyTo(channelData, { planeIndex: 0 });
            pcmDataArray.push(channelData);
          } else {
            let channelData = [];
            for (let c = 0; c < numChannels; c++) {
              let channelArray = new Float32Array(numFrames);
              frame.copyTo(channelArray, { planeIndex: c });
              channelData.push(channelArray);
            }
            let interleaved = new Float32Array(numFrames * numChannels);
            for (let i = 0; i < numFrames; i++) {
              for (let c = 0; c < numChannels; c++) {
                interleaved[i * numChannels + c] = channelData[c][i];
              }
            }
            pcmDataArray.push(interleaved);
          }
          frame.close();
        }
        let totalLength = pcmDataArray.reduce((sum, arr) => sum + arr.length, 0);
        let pcmFloat32 = new Float32Array(totalLength);
        let offset = 0;
        for (let arr of pcmDataArray) {
          pcmFloat32.set(arr, offset);
          offset += arr.length;
        }
        let pcmInt16 = floatTo16BitPCM(pcmFloat32);
        let wavBlob = encodeWAV(pcmInt16, sampleRate, numChannels);
        audioFrames = framesToKeep; // Retain overlap frames
        const mimeType = "audio/wav";
        const extension = "wav";
        const currentChunk = chunkNumber;
        uploadChunk(wavBlob, currentChunk, extension, mimeType, false, groupId)
          .then(result => {
            if (result && result.session_id) {
              console.log(`Chunk ${currentChunk} uploaded. Session ID: ${result.session_id}`);
              pollChunkTranscript(currentChunk, groupId);
            } else {
              console.log(`Chunk ${currentChunk} upload did not return a session ID; skipping polling.`);
            }
          })
          .catch(err => console.error(`Upload error for chunk ${currentChunk}:`, err));
        chunkNumber++;
      }
      
      // Upload function with API error status message
      async function uploadChunk(blob, currentChunkNumber, extension, mimeType, isLast = false, currentGroup) {
        const formData = new FormData();
        formData.append("file", blob, `chunk_${currentChunkNumber}.${extension}`);
        formData.append("group_id", currentGroup);
        formData.append("chunk_number", currentChunkNumber);
        formData.append("api_key", sessionStorage.getItem("openai_api_key"));
        if (isLast) {
          formData.append("last_chunk", "true");
        }
        try {
          const response = await fetch(`${backendUrl}/upload`, {
            method: "POST",
            body: formData
          });
          if (!response.ok) {
            updateStatusMessage("API error: " + response.statusText, "red");
            throw new Error("API error: " + response.statusText);
          }
          const result = await response.json();
          return result;
        } catch (error) {
          updateStatusMessage("Error uploading chunk " + currentChunkNumber + ": " + error, "red");
          throw error;
        }
      }
      
      function pollChunkTranscript(chunkNum, currentGroup) {
        pollingIntervals[chunkNum] = setInterval(async () => {
          if (groupId !== currentGroup) {
            clearInterval(pollingIntervals[chunkNum]);
            return;
          }
          try {
            const response = await fetch(`${backendUrl}/fetch_chunk`, {
              method: "POST",
              headers: {"Content-Type": "application/json"},
              body: JSON.stringify({ session_id: currentGroup, chunk_number: chunkNum })
            });
            if (response.status === 200) {
              const data = await response.json();
              transcriptChunks[chunkNum] = data.transcript;
              updateTranscriptionOutput();
              clearInterval(pollingIntervals[chunkNum]);
              if (manualStop && chunkNum === chunkNumber - 1) {
                clearInterval(completionTimerInterval);
                updateStatusMessage("Transcription finished!", "green");
              }
            } else {
              console.log(`Chunk ${chunkNum} transcript not ready yet.`);
            }
          } catch (err) {
            console.error(`Error polling for chunk ${chunkNum}:`, err);
          }
        }, 2000);
      }
      
      function updateTranscriptionOutput() {
        let sortedKeys = Object.keys(transcriptChunks).map(Number).sort((a, b) => a - b);
        let combinedTranscript = "";
        sortedKeys.forEach(key => {
          combinedTranscript += transcriptChunks[key] + " ";
        });
        document.getElementById("transcription").value = combinedTranscript.trim();
      }
      
      function finalPollAllChunks() {
        console.log("Starting final polling for all chunks...");
        for (let i = 1; i < chunkNumber; i++) {
          pollChunkTranscript(i, groupId);
        }
      }
      
      // Dynamic scheduling function for chunk processing using MIN and MAX durations.
      function scheduleChunk() {
        const elapsed = Date.now() - chunkStartTime;
        const timeSinceLast = Date.now() - lastFrameTime;
        if (elapsed >= MAX_CHUNK_DURATION) {
          processAudioChunk();
          chunkStartTime = Date.now();
          scheduleChunk();
        } else if (elapsed >= MIN_CHUNK_DURATION && timeSinceLast >= watchdogThreshold) {
          processAudioChunk();
          chunkStartTime = Date.now();
          scheduleChunk();
        } else {
          chunkTimeoutId = setTimeout(scheduleChunk, 500);
        }
      }
      
      const backendUrl = "https://whisper-dev-backend.fly.dev";
      
      document.getElementById("startButton").addEventListener("click", async () => {
        // Reset scheduling variables
        chunkStartTime = Date.now();
        lastFrameTime = Date.now();
        clearTimeout(chunkTimeoutId);
        
        manualStop = false;
        transcriptChunks = {};
        Object.values(pollingIntervals).forEach(interval => clearInterval(interval));
        pollingIntervals = {};
        updateStatusMessage("Recording...", "green");
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          groupId = Date.now().toString();
          chunkNumber = 1;
          audioFrames = [];
          recordingStartTime = Date.now();
          updateRecordingTimer();
          recordingTimerInterval = setInterval(updateRecordingTimer, 1000);
          
          const track = mediaStream.getAudioTracks()[0];
          const processor = new MediaStreamTrackProcessor({ track: track });
          audioReader = processor.readable.getReader();
          
          function readLoop() {
            audioReader.read().then(({ done, value }) => {
              if (done) {
                console.log("Audio track reading complete.");
                return;
              }
              lastFrameTime = Date.now();
              audioFrames.push(value);
              readLoop();
            }).catch(err => {
              console.error("Error reading audio frames:", err);
            });
          }
          readLoop();
          scheduleChunk();
          console.log("MediaStreamTrackProcessor started, reading audio frames...");
          document.getElementById("startButton").disabled = true;
          document.getElementById("stopButton").disabled = false;
          document.getElementById("pauseResumeButton").disabled = false;
          document.getElementById("pauseResumeButton").innerText = "Pause Recording";
          // Add pulsing animation to the recording indicator
          document.getElementById("recordIndicator").classList.add("recording");
        } catch (error) {
          updateStatusMessage("Microphone access error: " + error, "red");
        }
      });
      
      document.getElementById("stopButton").addEventListener("click", async () => {
        updateStatusMessage("Finishing transcription...", "blue");
        manualStop = true;
        // Clear dynamic scheduler and recording timer
        clearTimeout(chunkTimeoutId);
        clearInterval(recordingTimerInterval);
        stopMicrophone();
        // Reset scheduling variables to avoid interference with next recording
        chunkStartTime = 0;
        lastFrameTime = 0;
        // Remove pulsing animation and set indicator to grey
        document.getElementById("recordIndicator").classList.remove("recording");
        document.getElementById("recordIndicator").style.backgroundColor = "grey";
        // Final capture delay of 2 seconds
        await new Promise(resolve => setTimeout(resolve, 2000));
        await processAudioChunk();
        // Start the completion timer until the final transcript is received.
        completionStartTime = Date.now();
        completionTimerInterval = setInterval(() => {
          document.getElementById("transcribeTimer").innerText = "Completion Timer: " + formatTime(Date.now() - completionStartTime);
        }, 1000);
        finalPollAllChunks();
        document.getElementById("startButton").disabled = false;
        document.getElementById("stopButton").disabled = true;
        document.getElementById("pauseResumeButton").disabled = true;
        console.log("Recording stopped by user.");
      });
      
      document.getElementById("pauseResumeButton").addEventListener("click", () => {
        if (!mediaStream) return;
        const track = mediaStream.getAudioTracks()[0];
        if (track.enabled) {
          track.enabled = false;
          clearInterval(recordingTimerInterval);
          document.getElementById("pauseResumeButton").innerText = "Resume Recording";
          updateStatusMessage("Recording paused", "orange");
        } else {
          track.enabled = true;
          recordingStartTime = Date.now();
          recordingTimerInterval = setInterval(updateRecordingTimer, 1000);
          document.getElementById("pauseResumeButton").innerText = "Pause Recording";
          updateStatusMessage("Recording...", "green");
        }
      });
      
      /* --------------------------
         Custom Prompt and Note Generation Functionality
         -------------------------- */
      const promptSlotSelect = document.getElementById("promptSlot");
      const customPromptTextarea = document.getElementById("customPrompt");
      
      function hashString(str) {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
          const char = str.charCodeAt(i);
          hash = ((hash << 5) - hash) + char;
          hash |= 0;
        }
        return hash.toString();
      }
      
      function getPromptStorageKey(slot) {
        const apiKey = sessionStorage.getItem("openai_api_key") || "";
        const hashedApiKey = hashString(apiKey);
        return "customPrompt_" + hashedApiKey + "_" + slot;
      }
      
      function autoResize(textarea) {
        textarea.style.height = "auto";
        textarea.style.height = textarea.scrollHeight + "px";
      }
      
      function loadPromptForSlot(slot) {
        const key = getPromptStorageKey(slot);
        const storedPrompt = localStorage.getItem(key);
        customPromptTextarea.value = storedPrompt ? storedPrompt : "";
        autoResize(customPromptTextarea);
      }
      
      customPromptTextarea.addEventListener("input", () => {
        const currentSlot = promptSlotSelect.value;
        const key = getPromptStorageKey(currentSlot);
        localStorage.setItem(key, customPromptTextarea.value);
        autoResize(customPromptTextarea);
      });
      
      promptSlotSelect.addEventListener("change", () => {
        loadPromptForSlot(promptSlotSelect.value);
      });
      
      loadPromptForSlot(promptSlotSelect.value);
      
      document.getElementById("generateNoteButton").addEventListener("click", async () => {
        const transcriptionText = document.getElementById("transcription").value.trim();
        if (!transcriptionText) {
          alert("No transcription text available.");
          return;
        }
        const promptText = customPromptTextarea.value;
        const generatedNoteField = document.getElementById("generatedNote");
        generatedNoteField.value = "";
        const noteStartTime = Date.now();
        const noteTimerElement = document.getElementById("noteTimer");
        noteTimerElement.innerText = "Note Generation Timer: 0 sec";
        const noteTimerInterval = setInterval(() => {
          noteTimerElement.innerText = "Note Generation Timer: " + formatTime(Date.now() - noteStartTime);
        }, 1000);
        const apiKey = sessionStorage.getItem("openai_api_key");
        try {
          const response = await fetch("https://api.openai.com/v1/chat/completions", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              "Authorization": "Bearer " + apiKey
            },
            body: JSON.stringify({
              model: "gpt-4-turbo",
              messages: [
                { role: "system", content: promptText },
                { role: "user", content: transcriptionText }
              ],
              temperature: 0.7,
              stream: true
            })
          });
          if (!response.ok) {
             updateStatusMessage("API error during note generation: " + response.statusText, "red");
             throw new Error("API error during note generation: " + response.statusText);
          }
          const reader = response.body.getReader();
          const decoder = new TextDecoder("utf-8");
          let done = false;
          while (!done) {
            const { value, done: doneReading } = await reader.read();
            done = doneReading;
            const chunkValue = decoder.decode(value);
            const lines = chunkValue.split("\n").filter(line => line.trim() !== "");
            for (const line of lines) {
              if (line.startsWith("data: ")) {
                const jsonStr = line.replace("data: ", "").trim();
                if (jsonStr === "[DONE]") {
                  done = true;
                  break;
                }
                try {
                  const parsed = JSON.parse(jsonStr);
                  const textChunk = parsed.choices[0].delta?.content || "";
                  generatedNoteField.value += textChunk;
                  autoResize(generatedNoteField);
                } catch (err) {
                  console.error("Stream chunk parsing error:", err);
                }
              }
            }
          }
          clearInterval(noteTimerInterval);
          noteTimerElement.innerText = "Text generation completed!";
        } catch (error) {
          clearInterval(noteTimerInterval);
          generatedNoteField.value = "Error generating note: " + error;
          noteTimerElement.innerText = "";
          updateStatusMessage("Error generating note: " + error, "red");
        }
      });
      
    });
  </script>
</body>
</html>
